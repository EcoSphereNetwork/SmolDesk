# ðŸ§  Codex Folgeprompt â€“ Analyse der Projekt-Dokumentation & Erweiterte Teststabilisierung

## ðŸ”„ AUSGANGSLAGE

Die Testumgebung von **SmolDesk** wurde deutlich verbessert, jedoch bestehen weiterhin instabile oder fehlschlagende Tests rund um:

- WebRTC-FunktionalitÃ¤ten (`RTCPeerConnection`, `MediaStream`)
- BildschirmÃ¼bertragung (`this.mediaStream.getTracks is not a function`)
- Zusammenspiel von Tauri + Web + Systemfunktionen

Gleichzeitig wurde die globale Teststruktur konsolidiert, Mocks eingefÃ¼hrt und dokumentiert.

---

## ðŸŽ¯ ZIEL DIESES LAUFS

Codex, analysiere die gesamte Dokumentation von SmolDesk, um tieferes VerstÃ¤ndnis Ã¼ber:

- Architektur
- API-Design
- technische Ziele
- relevante Komponenten

zu erhalten â€“ und nutze dieses Wissen, um die noch fehlschlagenden Tests korrekt zu stabilisieren, zu verbessern oder gezielt auszulassen.

---

## âœ… AUFGABEN FÃœR CODEX

### 1. ðŸ“š ProjektverstÃ¤ndnis durch Dokumentation
- Analysiere folgende Dokumentationspfade vollstÃ¤ndig:
  - `./docs/docs/SmolDesk` â€“ besonders `development/*` `docs/docs/SmolDesk/README.md`
  - `./README.md` â€“ Hinweise zu Zielen, Komponentenstruktur, Feature-Flags

- Extrahiere:
  - Zielarchitektur (Frontend/Backend-Kommunikation, Tauri-Anbindung)
  - Nutzung von WebRTC, Signaling, Events
  - geplante oder beschriebene Teststrategien
- Dokumentiere eine Kurz-Zusammenfassung als `docs/docs/summary.project-insights.md`

### 2. ðŸ§ª Erweiterte Test-Stabilisierung
- Nutze das neue Wissen Ã¼ber Architektur & FunktionalitÃ¤t, um die Tests rund um **ScreenCapture & MediaStreams** zu verbessern:
  - Ersetze oder erweitere die bisherigen Mock-Implementierungen:
    ```ts
    globalThis.navigator.mediaDevices = {
      getDisplayMedia: vi.fn(() => Promise.resolve({ getTracks: () => [] })),
      getUserMedia: vi.fn(() => Promise.resolve({ getTracks: () => [] })),
    };
    ```
  - Erstelle ggf. `MockMediaStream`, `MockTrack` Klassen im Setup.
- PrÃ¼fe ob bestimmte Tests durch Feature-Flags deaktiviert sein sollten (temporÃ¤res `.skip()`), falls sie nur im echten Tauri-Context ausfÃ¼hrbar wÃ¤ren.
- ÃœberprÃ¼fe besonders:
  - `screen-capture.test.ts`
  - `connection.test.ts`
  - `enhanced-webrtc.test.ts`
  - `state-handlers.test.ts`

### 3. âœ… Verbesserte Teststrategie dokumentieren
- Lege eine Datei `docs/testing/strategy.md` an mit:
  - Ãœbersicht Ã¼ber alle Testarten im Projekt (Unit, Integration, Mock-based Simulation)
  - Hinweise zu realen vs. gemockten Laufumgebungen (Tauri vs. Node)
  - Warnhinweise bei flakey Tests und Feature-Simulationen

### 4. ðŸ§ª Tests ausfÃ¼hren & bewerten
- FÃ¼hre `npm run test:run` erneut aus
- PrÃ¼fe:
  - Bestehen alle WebRTC/Screen-Tests?
  - Sind Fehler reproduzierbar oder instabil?
- Bei Fehlern:
  - Erstelle pro Datei/Fehler ein GitHub Issue mit Beschreibung + Stacktrace
  - Falls Fix mÃ¶glich: Commit mit Titel wie:
    > âœ… Fix: WebRTC Test getTracks Mocked Correctly

---

## ðŸ§© HINWEISE

- Die Projekt-Dokumentation liefert **Kontext, den du zur Fehlerkorrektur brauchst** â€“ lies sie grÃ¼ndlich.
- Verwende synthetische Objekte nur dort, wo echte APIs fehlen â€“ vermeide Overmocking.
- Setze auf nachvollziehbare, dokumentierte Architekturentscheidungen.

---

ðŸ“Œ Starte jetzt mit:
```bash
# 1. Dokumentation lesen & analysieren
cat ./docs/*.md ./docs/wiki/**/*.md ./README.md

# 2. Teststruktur verbessern
npm run test:run
````

und dokumentiere deine Erkenntnisse in `docs/summary.project-insights.md`.
